---
title: "How Well We Exercise"
author: "Pilar Lafuente"
date: "11/12/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r caret_opts, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, fig.align = "center")
```


## 1. Background and Objective

The researchers were interesed in evaluate how well we do a particular activity when exercising based on the collected data from from accelerometers on the belt, forearm, arm, and dumbel. 

Six male participants aged between 20-28 years, with little weight lifting experience, were asked to perform barbell lifts correctly and incorrectly in 5 different ways ("classe" variable). The researchers made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

The participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 
- **Class A**: exactly according to the specification,
- **Class B**: throwing the elbows to the front,
- **Class C**: lifting the dumbbell only halfway, 
- **Class D**: lowering the dumbbell only halfway,
- **Class E**: throwing the hips to the front. 

Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. 

**Goal**: Use the collected data of the 6 participants to predict the manner in which they did the exercise. 

Evaluate the "classe" variable in the training set and use any of the other variables to predict whether the person is performing the exercise correctly.

*Paper*:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## 2. Load the data

Read the data:
```{r}
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")
```

and see the first first rows of the data (not show due to space limitations):
```{r eval=FALSE}
head(train)
head(test)
```

Let's evaluate the dimension of the data in the `train` set:
```{r}
dim(train)
```

and the `test` set:

```{r}
dim(test)
```

The train data contains 19622 rows and 160 columns. The test data includes 20 observations of the 160 variables.

From the 160 variables, the first 5 columns describes, the index (`X`), the name of the participant (`user_name`) and three columns with time information (`raw_timestamp_part_1`, `raw_timestamp_part_2` and `cvtd_timestamp`). The last column is the classification `classe` A-E.

As detailed in the publised paper, the researchers used a sliding window approach for feature extraction with lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. The columns `new_window` and `num_window` contain the information about the window.

In each step of the sliding window approach, the researchers calculated features on the Euler angles (**roll**, **pitch** and **yaw**), as well as the raw **accelerometer**, **gyroscope** and **magnetometer** readings for the three directions *x*, *y*, *z* and the **total acceleration**, for each of the four sensors (**belt**, **arm**, **dumbbell** and **forearm**). These gives a total of 13 observartions per sensor with a 52 raw observations in total.

The researchers calculated eight features: *mean*, *variance*, *standard deviation*, *max*, *min*, *amplitude*, *kurtosis* and *skewness* for each of the three Euler angles (**roll**, **pitch** and **yaw**) of the four sensors (**belt**, **arm**, **dumbbell** and **forearm**). This  generated a total 96 derived feature sets. Additionally, they calculated the variation of total acceleration (`var_total_accel`) per each sensor, giving a total of 100 features. We would have worked with these features, since they are statsitical calculations witin the temporal windows. However, since these features have no values in the `test` data set, we have exclude them in the analysis.

```{r eval=FALSE}
summary(test)
```


The selected features are the three Euler angles (**roll**, **pitch** and **yaw**),  the three directions, *x*, *y* and *z* of the **gyros**, **accel** and **magnet** (9 variables) and the total acceleration **total_accel** of the four sensors (**belt**, **arm**, **dumbbell** and **forearm**). This is 13 parameters per sensonsor, which gives a total of 52 features that we will include in our dataset.

```{r}
#install libraries
library(tidyverse)
library(dplyr)

#Select features to be included

selection <- c("user_name","^roll_","^pitch_","^yaw_","^gyros_","^accel_","^magnet_","^total_accel", "classe")
features <- paste(selection, collapse="|")
train_select <- train %>%
  select(matches(features))

test_select <- test %>%
  select(matches(features))

# Number of columns per each calculated feature
for (i in 1:length(selection)){
  cat(selection[i],": ",length(grep(selection[i],colnames(train_select))),"\n")
}
```

Evaluate if there are N/A values:
```{r}
# train set
sum(is.na(train_select))
cat("Dimension of the train data:", dim(train_select), "\n")

# test set
sum(is.na(train_select))
cat("Dimension of the test data:", dim(test_select), "\n")

```

There are no NA values.

Let's evaluate the structure of the data in the train set (not shown due to limited space):
```{r eval=FALSE}
str(train_select)
```

All are numeric variables, except the `user_name` and `classe`.

Classification of the exercise per person:

```{r}
table(train_select$user_name, train_select$classe)
```

Summary of the training_data (results not shown in the report, but evaluated):
```{r eval=FALSE}
summary(train_select)
```

## 3. Model

We will be using the caret package:

```{r}
library(caret)
set.seed(2019-11-11) # Set seed
```

Split the data in:

- Train (60%)
- Test (20%)
- Validation (20%)

```{r}
set.seed(2019-11-11) # Set seed
inTrain <- createDataPartition(y=train_select$classe, p=0.6, list=FALSE)
training <- train_select[inTrain,2:54]
rest <- train_select[-inTrain,2:54]
inTest <- createDataPartition(y=rest$classe, p=0.5, list=FALSE)
testing <- rest[inTest,]
validation <- rest[-inTest,]
cat("Dimensions training set:", dim(training),"\n")
cat("Dimensions testing set:", dim(testing),"\n")
cat("Validation set:", dim(validation),"\n")
```


First, we will standardize the training data to scale and center the values. We will use the standardized parameters from the training data:

```{r}
preprocessParam <- preProcess(training,method=c("center", "scale"))
print(preprocessParam)
trainTransf <- predict(preprocessParam, training)
testTransf <- predict(preprocessParam, testing)
valTransf <- predict(preprocessParam, validation)
allTransf <- predict(preprocessParam, train_select)

# We will also transform with the same parameters the test data that we will evaluate once we select the model
testFinal <- predict(preprocessParam, test_select)
```

This is a classification problem (the outcome is a factor with 5 levels), so we will use supervised classification models:

- **Classification and regression trees**:
```{r cache = TRUE}
set.seed(2019-11-11) # Set seed
modTree <- train(classe~., method="rpart", data=trainTransf)
#print(modelTrees$finalModel)
library(rattle)
fancyRpartPlot(modTree$finalModel)
predTree <- predict(modTree, newdata=testTransf)
cmTree <- confusionMatrix(predTree, testTransf$classe)
cmTree
accTree <- cmTree$overall['Accuracy']
accTree
```

In the tree, the predictors that define the different branches are **roll_belt** (mainyly to split the E group), **pitch_forearm**, **magnet_dumbbbell_y** (allows to classify the B group), **roll_forearm** (Separation of part of the C group). There is no node that separates for the D classification.

Accuracy is very low, only 

```{r}
print(accTree)
```

Let's evaluate the correlation between predictors:

```{r}
Mcorr <- abs(cor(trainTransf[,-53])) # Correlation matrix with absolute values 
diag(Mcorr) <- 0 # Transform diagonal of the correlation matrix to zero
which(Mcorr > 0.8) # Print the correlation matrix values > 0.8
```

We can see that there are many predictors that have high correlation values. To reduce the dimension of variables to analyze, we will perform a Principal Component Analysis (PCA) and we will select the components that explain up to 90% of the variability:

```{r}
#Evaluate the possible PCA components
set.seed(2019-11-11) # Set seed
train_prcomp <- prcomp(trainTransf[,-53])
summary(train_prcomp)
```

The first 18 PCA explain 90% of the variance. We will these 18 PCAs to perform the transformation: 

```{r}
set.seed(2019-11-11) # Set seed
#Trasnform the train dataset
prePCA <- preProcess(trainTransf[,-53], method="pca",pcaComp=18)
trainPCA <- predict(prePCA, trainTransf[,-53])
testPCA <- predict(prePCA, testTransf[,-53])
valPCA <- predict(prePCA, valTransf[,-53])

#Trasnform the test dataset

testFinalPCA <- predict(prePCA, testFinal)
```

Let's re-do the **Tree** method with the PCAs, although the splitting nodes will not have easy interpretation.

```{r cache = TRUE}
set.seed(2019-11-11) # Set seed
modTreePCA <- train(x=trainPCA, y=trainTransf$classe, method="rpart")
#print(modelTrees$finalModel)
fancyRpartPlot(modTreePCA$finalModel)
predTreePCA <- predict(modTreePCA, newdata=testPCA)
cmTreePCA <- confusionMatrix(predTreePCA, testTransf$classe)
cmTreePCA
accTreePCA <- cmTreePCA$overall['Accuracy']
accTreePCA
```

The classification tree at this level has very low accuracy,
```{r}
print(accTreePCA)
```


so we will explore other methods.

The next methods will be performed on the PCA transformed data sets:

- **Bagging**:
```{r cache = TRUE}

library(party)
set.seed(2019-11-11) # Set seed

classe <- trainTransf$classe

# Model
modBag <- bag(trainPCA, classe, B=10,
               bagControl = bagControl(fit = ctreeBag$fit,
                                       predict=ctreeBag$pred,
                                       aggregate=ctreeBag$aggregate)
               )
# Predict on test extracted data
predBag <- predict(modBag, newdata=testPCA)
# Confusion matrix
cmBag <- confusionMatrix(predBag, testTransf$classe)
cmBag
accBag <- cmBag$overall['Accuracy']
accBag
```


- **Random forest**:

The model includes the *cross validation* with 5 repeats:
```{r cache = TRUE}
set.seed(2019-11-11) # Set seed
#Random Forest Model with 5 fold cross validation
modRF <- train(x=trainPCA, y=classe, method="rf", prox=TRUE, 
               trControl= trainControl(method="cv",number=5, verboseIter = TRUE),  # 5 fold cross validation
               allowParallel=TRUE)

#Predictions
predRF <- predict(modRF, newdata=testPCA)

#Confusion Matrix
cmRF <- confusionMatrix(predRF, testTransf$classe)
cmRF
accRF <- cmRF$overall['Accuracy']
accRF
```

Accuracy of:

```{r}
print(accRF)
```

- **Boosting with trees**:

The model includes the *cross validation* with 5 repeats:
```{r cache = TRUE}
set.seed(2019-11-11) # Set seed
#Boosting with Trees with 5 fold cross validation
modBT <- train(x=trainPCA, y=classe, method="gbm", 
               trControl= trainControl(method="cv",number=5, verboseIter = TRUE),  # 5 fold cross validation
               verbose=FALSE)

# Prediction
predBT <- predict(modBT, newdata=testPCA)

#Confusion Matrix
cmBT <- confusionMatrix(predBT, testTransf$classe)
cmBT

accBT <- cmBT$overall['Accuracy']
accBT
```

Accuracy of:
```{r}
print(accBT)
```


- **Combining predictors** from *baggin*, *boosting* and *random forest*:
```{r cache = TRUE}
set.seed(2019-11-11) # Set seed
predDF <-  data.frame(predBag, predRF, predBT, classe=testTransf$classe)
modCM <- train(classe~., method="rf", data=predDF,
               trControl= trainControl(method="cv",number=5, verboseIter = TRUE),
               allowParallel=TRUE)
predCM <- predict(modCM, predDF)
cmCM <- confusionMatrix(predCM, testTransf$classe)
cmCM
accCM <- cmCM$overall['Accuracy']
accCM
```

Accuracy:
```{r}
print(accCM)
```

, the same than the random forest method.

Compare accuracy of all the methods:
```{r}
methods <- c("Tree", "Bagging", " Random Forest", "Boosting", "Combined Methods")
acc <- c(accTree,accBag,accRF,accBT,accCM)
accDF <- data.frame(methods,round(acc,3))
accDF
```

The most accurate method is the **Random Forest** and the **Combined Methods** with the same accuracy.

On validation dataset:
```{r}
set.seed(2019-11-11) # Set seed
#Tree
valPredTree <- predict(modTree,valTransf) # The tree model has been built with all the predictors
valAccTree <- confusionMatrix(valPredTree, valTransf$classe)$overall['Accuracy']

#Baggin
valPredBag <- predict(modBag,valPCA)
valAccBag <- confusionMatrix(valPredBag, valTransf$classe)$overall['Accuracy']
  
#Random Forest
valPredRF <- predict(modRF,valPCA)
valAccRF <- confusionMatrix(valPredRF, valTransf$classe)$overall['Accuracy']

#BT
valPredBT <- predict(modBT,valPCA)
valAccBT <- confusionMatrix(valPredBT, valTransf$classe)$overall['Accuracy']

#Combined methods
valPred <- data.frame(predBag=valPredBag, predRF=valPredRF, predBT=valPredBT)
valPredCM <- predict(modCM, valPred)
valAccCM <- confusionMatrix(valPredCM, valTransf$classe)$overall['Accuracy']

#Accuracy table
valAcc <- c(valAccTree, valAccBag, valAccRF, valAccBT, valAccCM)
valAccDF <- data.frame(methods,round(valAcc,3))
valAccDF
```

We select the **Random Forest** as the model to predict the quality of weight lifting exercise using the variables collected from the wearables.

## 4. In Sample and Out Sample Error Evaluation (missclassification)

- In sample error: with *training* data:
```{r}
set.seed(2019-11-11) # Set seed
# Using the confusion matrix of the Random Forest model for the training data
predRFtrain <- predict(modRF, trainPCA)
cmRF_train <- table(trainTransf$classe,predRFtrain)
cmRF_train
in_err <- 1-sum(diag(cmRF_train)/sum(cmRF_train))
in_err
```

- Out sample error: with *testing* data:
```{r}
set.seed(2019-11-11) # Set seed
# Using the confusion matrix of the Random Forest model for the training data
predRFtest <- predict(modRF, testPCA)
cmRF_test <- table(testTransf$classe,predRFtest)
cmRF_test
out_err_test <- 1-sum(diag(cmRF_test)/sum(cmRF_test))
out_err_test
```


- Out sample error: with *validation* data:
```{r}
set.seed(2019-11-11) # Set seed
# Using the confusion matrix of the Random Forest model for the training data
predRFval <- predict(modRF, valPCA)
cmRF_val <- table(valTransf$classe,predRFval)
cmRF_val
out_err_val <- 1-sum(diag(cmRF_test)/sum(cmRF_test))
out_err_val
```

Out of sample error is about 3% both for the testing and validation sets.

## 5. New Data Predictions

With the stadardized test data, we predict the class of the exercise with the selected model:

```{r}
testPredict <- predict(modRF, testFinalPCA)
testPredict
```


